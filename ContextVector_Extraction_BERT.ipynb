{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ContextVector_Extraction_BERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOdrn91F/f0VmAv13HssicT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QoNFmRf_-VZ","executionInfo":{"status":"ok","timestamp":1614327366631,"user_tz":-540,"elapsed":1165,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"88715eb8-5c8e-4b6a-81b4-3f61d3848d2b"},"source":["# mount google drive \r\n","\r\n","import os, sys \r\n","from google.colab import drive\r\n","\r\n","drive.mount('/content/gdrive')\r\n","%cd /content/gdrive/MyDrive/가사유사도기반추천"],"execution_count":32,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/가사유사도기반추천\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2mt67rs4ABX2","executionInfo":{"status":"ok","timestamp":1614327369812,"user_tz":-540,"elapsed":4327,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"4c87e250-516f-4028-8f5f-c1ef06d40975"},"source":["# ###0. BERT 설치\r\n","\r\n","# 필요 패키지 설치\r\n","get_ipython().system('pip install transformers')"],"execution_count":33,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.3.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0p5Os6xLAVsw","executionInfo":{"status":"ok","timestamp":1614327369814,"user_tz":-540,"elapsed":4322,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","from torch.utils.data import TensorDataset, DataLoader\r\n","\r\n","from transformers import BertTokenizer\r\n","from transformers import BertForPreTraining, BertConfig, BertModel\r\n","\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import time\r\n","import datetime\r\n","import os"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yURwxeEHAllz","executionInfo":{"status":"ok","timestamp":1614327369816,"user_tz":-540,"elapsed":4318,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"13106284-679a-4be1-fa8f-4486bcf698a2"},"source":["# Check GPU\r\n","print(\"GPU : \",torch.cuda.get_device_name(0))\r\n","\r\n","# Set GPU\r\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":35,"outputs":[{"output_type":"stream","text":["GPU :  Tesla P100-PCIE-16GB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CdvjQ4X-AmFu","executionInfo":{"status":"ok","timestamp":1614327369817,"user_tz":-540,"elapsed":4312,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["# ### 1. 인풋 파이프라인 세팅\r\n","\r\n","# csv 데이터셋 파일 경로\r\n","PATH = './data/remove_meaningless.csv'\r\n","\r\n","# 판다스로 dataset 불러오기\r\n","dataset = pd.read_csv(PATH)\r\n","\r\n","\r\n","# 최대 입력 시퀀스 길이 설정\r\n","MAX_LEN = 512\r\n","# 배치 사이즈 설정\r\n","batch_size = 64"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":361},"id":"9p81D1oUApRG","executionInfo":{"status":"error","timestamp":1614327379430,"user_tz":-540,"elapsed":13919,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"a63278ea-f512-468c-b3b2-7e060870fcf6"},"source":["def make_input_from_dataset(dataset, MAX_LEN=512, batch_size=64) :\r\n","  # [CLS] = 101, [SEP] = 102 in bert-base-multilingual-cased vocab\r\n","  # [CLS], [SEP] 토큰 앞뒤로 붙히기\r\n","  document_bert = [\"[CLS] \" + str(i) + \" [SEP]\" for i in dataset['lyrics']]\r\n","\r\n","  # Multilingual BERT 토크나이저 소환\r\n","  tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased', do_lower_case=False)\r\n","\r\n","  # 토크나이징\r\n","  document_bert = [tokenizer.tokenize(j) for j in document_bert]\r\n","\r\n","  # vocab index로 변환\r\n","  input_ids = [tokenizer.convert_tokens_to_ids(i) for i in document_bert]\r\n","\r\n","  # 입력 시퀀스 만들기\r\n","  input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\",padding=\"post\")\r\n","\r\n","  # 어텐션 마스크 세팅\r\n","  attention_masks = []\r\n","\r\n","  # Padding = 0, Not Padding = 1 / Not trained for Padding\r\n","  for seq in input_ids:\r\n","      seq_mask = [float(i>0) for i in seq]\r\n","      attention_masks.append(seq_mask)\r\n","\r\n","  # 파이토치 텐서로 변환\r\n","  dataset_inputs = torch.tensor(input_ids, dtype=torch.long)\r\n","  dataset_masks = torch.tensor(attention_masks, dtype=torch.long)\r\n","\r\n","  # input_ids, attention masks 파이토치 데이터로더로 묶기\r\n","  final_data = TensorDataset(dataset_inputs, dataset_masks)\r\n","  final_dataloader = DataLoader(final_data, batch_size=batch_size)\r\n","\r\n","  return final_dataloader\r\n","\r\n","\r\n","final_dataloader = make_input_from_dataset(dataset,MAX_LEN,batch_size)"],"execution_count":37,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-37-6deca2f6fffd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0mfinal_dataloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_input_from_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMAX_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-37-6deca2f6fffd>\u001b[0m in \u001b[0;36mmake_input_from_dataset\u001b[0;34m(dataset, MAX_LEN, batch_size)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# 토크나이징\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mdocument_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_bert\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# vocab index로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-37-6deca2f6fffd>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0;31m# 토크나이징\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0mdocument_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocument_bert\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m# vocab index로 변환\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0mno_split_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mtokenized_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_on_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_split_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36msplit_on_tokens\u001b[0;34m(tok_list, text)\u001b[0m\n\u001b[1;32m    334\u001b[0m                     (\n\u001b[1;32m    335\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                     )\n\u001b[1;32m    338\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    334\u001b[0m                     (\n\u001b[1;32m    335\u001b[0m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique_no_split_tokens\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 336\u001b[0;31m                         \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    337\u001b[0m                     )\n\u001b[1;32m    338\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_tokenize\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0msplit_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_basic_tokenize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnever_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_special_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# If the token is part of the never_split set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36mtokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;31m# union() returns a new set by concatenating the two sets.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0mnever_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnever_split\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnever_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m         \u001b[0;31m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/tokenization_bert.py\u001b[0m in \u001b[0;36m_clean_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xFFFD\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_is_control\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 497\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0m_is_whitespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    498\u001b[0m                 \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"FH1LEUlJAsYW","executionInfo":{"status":"aborted","timestamp":1614327379418,"user_tz":-540,"elapsed":13901,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["# ### 2. BERT\r\n","\r\n","# bert-base-multilingual-cased 소환\r\n","model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")\r\n","\r\n","\r\n","# 시간 표시 함수\r\n","def format_time(elapsed):\r\n","    elapsed_rounded = int(round((elapsed)))\r\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n","\r\n","\r\n","# 최종 vectors 배열로\r\n","final_vectors = np.zeros([1,768],dtype=float)\r\n","\r\n","t0 = time.time()\r\n","model.eval()\r\n","\r\n","# model gpu로 보내기\r\n","model.to(device)\r\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"szxQbhBoAuhy","executionInfo":{"status":"aborted","timestamp":1614327379420,"user_tz":-540,"elapsed":13895,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["# 데이터로더에서 배치만큼 반복하여 가져옴\r\n","for step, batch in enumerate(final_dataloader):\r\n","    # 경과 정보 표시\r\n","    if step % 20 == 0 and not step == 0:\r\n","        elapsed = format_time(time.time() - t0)\r\n","        print('Batch {:>5,} / {:>5,} - Time Running.. {:}.'.format(step, len(final_dataloader), elapsed))\r\n","\r\n","    # 배치를 GPU에 넣음\r\n","    batch = tuple(t.to(device) for t in batch)\r\n","    \r\n","    b_input_ids, b_input_mask = batch\r\n","    \r\n","    with torch.no_grad():     \r\n","        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\r\n","\r\n","    # CPU로 데이터 이동\r\n","    vectors = outputs[1].detach().cpu().numpy()\r\n","\r\n","    # 넘파이 배열로\r\n","    final_vectors = np.concatenate((final_vectors,vectors),axis=0)\r\n","    del vectors\r\n","\r\n","print(\"Done.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A38cYyawAwO6","executionInfo":{"status":"aborted","timestamp":1614327379422,"user_tz":-540,"elapsed":13890,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["# 넘파이 배열 초기화 당시 첫행 제거\r\n","final_vectors = final_vectors[1:]\r\n","\r\n","\r\n","# 불러온 데이터셋이랑 최종 넘파이 배열 레코드 갯수 확인\r\n","try:\r\n","  if len(dataset) != len(final_vectors) : \r\n","      raise Exception('Records of Dataset and Final vector shape is Not match.')\r\n","  print(\"최종 Context Vector Shape\",final_vectors.shape)\r\n","except Exception as e:\r\n","  print('Exception is occured. ', e)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VbbUscPvAx2a","executionInfo":{"status":"aborted","timestamp":1614327379424,"user_tz":-540,"elapsed":13885,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["## 최종 넘파이 배열 저장\r\n","np.save('./vec/mul_vec_ts',final_vectors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0qMnDxwCA4Y-","executionInfo":{"status":"aborted","timestamp":1614327379425,"user_tz":-540,"elapsed":13878,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["print(final_vectors)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X0_VamG6Cnbi","executionInfo":{"status":"aborted","timestamp":1614327379428,"user_tz":-540,"elapsed":13871,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":[""],"execution_count":null,"outputs":[]}]}