{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ContextVector_Extraction_KoBERT.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPLCoHYrg9Yh7MtSfGRRFIV"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8kKamQaw7Ba3","executionInfo":{"status":"ok","timestamp":1614327412306,"user_tz":-540,"elapsed":1259,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"dd299433-de6a-486d-c6a7-1fe404dc3747"},"source":["# mount google drive \r\n","\r\n","import os, sys \r\n","from google.colab import drive\r\n","\r\n","drive.mount('/content/gdrive')\r\n","%cd /content/gdrive/MyDrive/가사유사도기반추천"],"execution_count":17,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n","/content/gdrive/MyDrive/가사유사도기반추천\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":875},"id":"tfZOBSj92kxM","executionInfo":{"status":"ok","timestamp":1614327417818,"user_tz":-540,"elapsed":6756,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"843393d1-be7f-4b98-c022-70878a91abda"},"source":["#!/usr/bin/env python\r\n","# coding: utf-8\r\n","\r\n","# ## Dependancy\r\n","# - KoBERT\r\n","# - Python >= 3.6 \r\n","# - transformers >= 3.5\r\n","# - sentencepiece >= 0.1.6\r\n","# - Pytorch >= 1.7\r\n","# - Tensorflow >= 2.0.0\r\n","\r\n","# ###0. KoBERT 설치\r\n","\r\n","# KoBERT 설치 - vocab_size 8002, 600M step Training\r\n","get_ipython().system('git clone https://github.com/SKTBrain/KoBERT.git')\r\n","get_ipython().system('pip install -r ./KoBERT/requirements.txt')\r\n","get_ipython().system('pip install ./KoBERT')\r\n","get_ipython().system('ls ./KoBERT')"],"execution_count":18,"outputs":[{"output_type":"stream","text":["fatal: destination path 'KoBERT' already exists and is not an empty directory.\n","Requirement already satisfied: torch>=1.7.0 in /usr/local/lib/python3.7/dist-packages (from -r ./KoBERT/requirements.txt (line 1)) (1.7.0+cu101)\n","Requirement already satisfied: mxnet>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from -r ./KoBERT/requirements.txt (line 2)) (1.7.0.post2)\n","Requirement already satisfied: gluonnlp>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from -r ./KoBERT/requirements.txt (line 3)) (0.10.0)\n","Requirement already satisfied: sentencepiece>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from -r ./KoBERT/requirements.txt (line 4)) (0.1.95)\n","Requirement already satisfied: onnxruntime>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from -r ./KoBERT/requirements.txt (line 5)) (1.6.0)\n","Requirement already satisfied: transformers<4 in /usr/local/lib/python3.7/dist-packages (from -r ./KoBERT/requirements.txt (line 6)) (3.5.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r ./KoBERT/requirements.txt (line 1)) (1.19.5)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r ./KoBERT/requirements.txt (line 1)) (3.7.4.3)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r ./KoBERT/requirements.txt (line 1)) (0.16.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.7/dist-packages (from torch>=1.7.0->-r ./KoBERT/requirements.txt (line 1)) (0.6)\n","Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->-r ./KoBERT/requirements.txt (line 2)) (2.23.0)\n","Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from mxnet>=1.4.0->-r ./KoBERT/requirements.txt (line 2)) (0.8.4)\n","Requirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->-r ./KoBERT/requirements.txt (line 3)) (0.29.21)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from gluonnlp>=0.6.0->-r ./KoBERT/requirements.txt (line 3)) (20.9)\n","Requirement already satisfied: protobuf in /usr/local/lib/python3.7/dist-packages (from onnxruntime>=0.3.0->-r ./KoBERT/requirements.txt (line 5)) (3.12.4)\n","Requirement already satisfied: tokenizers==0.9.3 in /usr/local/lib/python3.7/dist-packages (from transformers<4->-r ./KoBERT/requirements.txt (line 6)) (0.9.3)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers<4->-r ./KoBERT/requirements.txt (line 6)) (0.0.43)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<4->-r ./KoBERT/requirements.txt (line 6)) (2019.12.20)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<4->-r ./KoBERT/requirements.txt (line 6)) (4.41.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<4->-r ./KoBERT/requirements.txt (line 6)) (3.0.12)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r ./KoBERT/requirements.txt (line 2)) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r ./KoBERT/requirements.txt (line 2)) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r ./KoBERT/requirements.txt (line 2)) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet>=1.4.0->-r ./KoBERT/requirements.txt (line 2)) (3.0.4)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->gluonnlp>=0.6.0->-r ./KoBERT/requirements.txt (line 3)) (2.4.7)\n","Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime>=0.3.0->-r ./KoBERT/requirements.txt (line 5)) (1.15.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf->onnxruntime>=0.3.0->-r ./KoBERT/requirements.txt (line 5)) (53.0.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4->-r ./KoBERT/requirements.txt (line 6)) (1.0.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers<4->-r ./KoBERT/requirements.txt (line 6)) (7.1.2)\n","Processing ./KoBERT\n","Building wheels for collected packages: kobert\n","  Building wheel for kobert (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for kobert: filename=kobert-0.1.2-cp37-none-any.whl size=12708 sha256=c8ad9779194f16bd0d755af34bd5b9d47a01d4665664a0a421202f94f89c3ddb\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-t353ad32/wheels/1d/d2/2e/7356b12ceb3f85796b88b2fbd81316f05311a39058ae9aa052\n","Successfully built kobert\n","Installing collected packages: kobert\n","  Found existing installation: kobert 0.1.2\n","    Uninstalling kobert-0.1.2:\n","      Successfully uninstalled kobert-0.1.2\n","Successfully installed kobert-0.1.2\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["kobert"]}}},"metadata":{"tags":[]}},{"output_type":"stream","text":["imgs  kobert  LICENSE  logs  README.md\trequirements.txt  scripts  setup.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7HhAO2LE645n","executionInfo":{"status":"ok","timestamp":1614327417821,"user_tz":-540,"elapsed":6749,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["import torch\r\n","import torch.nn as nn\r\n","from torch.utils.data import TensorDataset, DataLoader\r\n","\r\n","from kobert.pytorch_kobert import get_pytorch_kobert_model\r\n","from kobert.utils import get_tokenizer\r\n","\r\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n","\r\n","import sentencepiece as spm\r\n","\r\n","import pandas as pd\r\n","import numpy as np\r\n","import time\r\n","import datetime"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uIwPqu5U7QxL","executionInfo":{"status":"ok","timestamp":1614327417822,"user_tz":-540,"elapsed":6743,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"bf22faec-3938-4c8e-a1d9-111259a40201"},"source":["# Check GPU\r\n","print(\"GPU : \",torch.cuda.get_device_name(0))\r\n","\r\n","# Set GPU\r\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\""],"execution_count":20,"outputs":[{"output_type":"stream","text":["GPU :  Tesla T4\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ePRyLUCP7UTa","executionInfo":{"status":"ok","timestamp":1614327418607,"user_tz":-540,"elapsed":7519,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["# ### 1. 인풋 파이프라인 세팅\r\n","\r\n","# csv 데이터셋 파일 경로\r\n","PATH = './data/remove_meaningless.csv'\r\n","\r\n","# 판다스로 dataset 불러오기\r\n","dataset = pd.read_csv(PATH)\r\n","\r\n","# 최대 입력 시퀀스 길이 설정\r\n","MAX_LEN = 512\r\n","# 배치 사이즈 설정\r\n","batch_size = 64"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_-GF4TLX7jim","executionInfo":{"status":"ok","timestamp":1614327423905,"user_tz":-540,"elapsed":12810,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"d1453147-47e9-4960-e004-1e051a1bb423"},"source":["def make_input_from_dataset(dataset, MAX_LEN=512, batch_size=64) :\r\n","  # 구글 센텐스피스 토크나이저 소환\r\n","  tokenizer = spm.SentencePieceProcessor()\r\n","\r\n","  # KoBERT 토크나이저 할당\r\n","  tokenizer.load(get_tokenizer())\r\n","\r\n","  # 토크나이징\r\n","  document_bert = [tokenizer.encode_as_ids(str(i)) for i in dataset['lyrics']]\r\n","\r\n","  # 맨 앞에 [CLS], 맨 뒤에 [SEP] 추가\r\n","  # [UNK]=0, [PAD]=1, [CLS]=2, [SEP]=3 in KoBERT Vocab\r\n","  for j in document_bert:\r\n","    j.insert(0,2)\r\n","    j.append(3)\r\n","\r\n","  # 입력 시퀀스 만들기\r\n","  input_ids = pad_sequences(document_bert, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\",padding=\"post\",value=1)\r\n","\r\n","  # 어텐션 마스크 세팅\r\n","  attention_masks = []\r\n","\r\n","  # Padding = 0, Not Padding = 1 / Not trained for Padding\r\n","  for seq in input_ids:\r\n","      seq_mask = [float(i!=1) for i in seq]\r\n","      attention_masks.append(seq_mask)\r\n","\r\n","  # 파이토치 텐서로 변환\r\n","  dataset_inputs = torch.tensor(input_ids, dtype=torch.long)\r\n","  dataset_masks = torch.tensor(attention_masks, dtype=torch.long)\r\n","\r\n","  # input_ids, attention masks 묶어서 파이토치 데이터로더로 변환\r\n","  final_data = TensorDataset(dataset_inputs, dataset_masks)\r\n","  final_dataloader = DataLoader(final_data, batch_size=batch_size)\r\n","\r\n","  return final_dataloader\r\n","\r\n","\r\n","final_dataloader = make_input_from_dataset(dataset,MAX_LEN,batch_size)\r\n"],"execution_count":22,"outputs":[{"output_type":"stream","text":["using cached model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KmqXi4O07k8V","executionInfo":{"status":"ok","timestamp":1614327429617,"user_tz":-540,"elapsed":18514,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"17c643f5-ccdc-46f5-c070-6203472c2df5"},"source":["# ### 2. KoBERT\r\n","\r\n","\r\n","# KoBERT Model 소환\r\n","KoBERT_model, _  = get_pytorch_kobert_model()\r\n","\r\n","# Pre-training된 KoBERT model에서 [CLS] vector만 쉽게 뽑을 수 있도록 클래스 재정의\r\n","class Model(nn.Module):\r\n","  global KoBERT_model # KoBERT\r\n","  def __init__(self):\r\n","    super(Model, self).__init__()\r\n","    self.bert = KoBERT_model #KoBERT\r\n","    \r\n","  def forward(self,ids, mask):\r\n","    _, pooled_output = self.bert(input_ids=ids, attention_mask=mask)\r\n","    return pooled_output\r\n","\r\n","model = Model()"],"execution_count":23,"outputs":[{"output_type":"stream","text":["using cached model\n","using cached model\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRY3BFHM7snL","executionInfo":{"status":"ok","timestamp":1614327429622,"user_tz":-540,"elapsed":18510,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"47c409ac-2aa8-43a5-e55a-41bf27c16c5c"},"source":["# 시간 표시 함수\r\n","def format_time(elapsed):\r\n","    elapsed_rounded = int(round((elapsed)))\r\n","    return str(datetime.timedelta(seconds=elapsed_rounded))\r\n","\r\n","# 최종 vectors 배열로\r\n","final_vectors = np.zeros([1,768],dtype=float)\r\n","\r\n","t0 = time.time()\r\n","model.eval()"],"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Model(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(8002, 768, padding_idx=1)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"79zlVHzj8ATW","executionInfo":{"status":"ok","timestamp":1614327876722,"user_tz":-540,"elapsed":465602,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"dde35fe2-4762-468b-a033-36665251023e"},"source":["# model gpu로 보내기\r\n","model.to(device)\r\n","\r\n","# 데이터로더에서 배치만큼 반복하여 가져옴\r\n","for step, batch in enumerate(final_dataloader):\r\n","    # 경과 정보 표시\r\n","    if step % 20 == 0 and not step == 0:\r\n","        elapsed = format_time(time.time() - t0)\r\n","        print('Batch {:>5,} / {:>5,} - Time Running.. {:}.'.format(step, len(final_dataloader), elapsed))\r\n","\r\n","    # 배치를 GPU에 넣음\r\n","    batch = tuple(t.to(device) for t in batch)\r\n","    \r\n","    b_input_ids, b_input_mask = batch\r\n","    \r\n","    with torch.no_grad():     \r\n","        outputs = model(b_input_ids, b_input_mask)\r\n","    \r\n","    # CPU로 데이터 이동\r\n","    vectors = outputs.detach().cpu().numpy()\r\n","\r\n","    # 넘파이 배열로\r\n","    final_vectors = np.concatenate((final_vectors,vectors),axis=0)\r\n","    del vectors\r\n","\r\n","print(\"Done.\")"],"execution_count":25,"outputs":[{"output_type":"stream","text":["Batch    20 /   169 - Time Running.. 0:00:52.\n","Batch    40 /   169 - Time Running.. 0:01:45.\n","Batch    60 /   169 - Time Running.. 0:02:38.\n","Batch    80 /   169 - Time Running.. 0:03:32.\n","Batch   100 /   169 - Time Running.. 0:04:26.\n","Batch   120 /   169 - Time Running.. 0:05:19.\n","Batch   140 /   169 - Time Running.. 0:06:12.\n","Batch   160 /   169 - Time Running.. 0:07:06.\n","Done.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5yLmX45b-ZaT","executionInfo":{"status":"ok","timestamp":1614327876727,"user_tz":-540,"elapsed":465599,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"4053d28d-bbe3-4e18-fccc-f1da285a275a"},"source":["# 넘파이 배열 초기화 당시 첫행 제거\r\n","final_vectors = final_vectors[1:]\r\n","\r\n","# 불러온 데이터셋이랑 최종 넘파이 배열 레코드 갯수 확인\r\n","try:\r\n","  if len(dataset) != len(final_vectors) : \r\n","      raise Exception('Records of Dataset and Final vector shape is Not match.')\r\n","  print(\"최종 Context Vector Shape\",final_vectors.shape)\r\n","except Exception as e:\r\n","  print('Exception is occured. ', e)\r\n","\r\n"],"execution_count":26,"outputs":[{"output_type":"stream","text":["최종 Context Vector Shape (10757, 768)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LN8po28t_y05","executionInfo":{"status":"ok","timestamp":1614327876728,"user_tz":-540,"elapsed":465593,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":["## 최종 넘파이 배열 저장\r\n","np.save('./vec/ko_vec_rm',final_vectors)"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0LbDclOL_z5g","executionInfo":{"status":"ok","timestamp":1614327876729,"user_tz":-540,"elapsed":465588,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}},"outputId":"82ae20a6-705e-4a6c-c22e-f8660fdc6cb5"},"source":["print(final_vectors)"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[[-0.01166219 -0.04060391 -0.46878538 ... -0.00192322 -0.02360824\n","  -0.0363199 ]\n"," [-0.04199397 -0.04072817 -0.49069458 ...  0.01971427  0.04380579\n","   0.02465651]\n"," [ 0.01055787 -0.02890142 -0.07691309 ...  0.07225733 -0.03135946\n","  -0.01188726]\n"," ...\n"," [-0.01127518 -0.04588813 -0.43267262 ... -0.02362569 -0.0883085\n","  -0.04459684]\n"," [ 0.01160353 -0.00202722 -0.41084278 ... -0.03555009 -0.07257079\n","  -0.06399281]\n"," [-0.00685424 -0.07076862 -0.26634115 ...  0.02750332 -0.09353186\n","  -0.05930189]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"nar8VPf2BJ5w","executionInfo":{"status":"ok","timestamp":1614327876731,"user_tz":-540,"elapsed":465583,"user":{"displayName":"구선민","photoUrl":"","userId":"18188373089497760921"}}},"source":[""],"execution_count":28,"outputs":[]}]}